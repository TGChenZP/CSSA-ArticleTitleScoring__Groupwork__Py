{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import LlamaTokenizer, LlamaModel\n",
    "import torch\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 19260817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw data\n",
    "df1 = pd.read_excel(\"../data/raw/小助手数据采集.xlsx\", sheet_name=\"Sheet1\")\n",
    "df2 = pd.read_excel(\"../data/raw/小助手数据采集.xlsx\", sheet_name=\"Sheet2\")\n",
    "df3 = pd.read_excel(\"../data/raw/小助手数据采集.xlsx\", sheet_name=\"Sheet3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return emoji.replace_emoji(text, replace='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting and renaming columns\n",
    "df1.drop(0,inplace=True)\n",
    "df2.drop(0,inplace=True)\n",
    "df3.drop(0,inplace=True)\n",
    "\n",
    "# renmae for concat\n",
    "df1 = df1.rename(columns={\"2021.3-2022.3\": \"Time\"})\n",
    "df2 = df2.rename(columns={\"2022.4-2023.3\": \"Time\"})\n",
    "df3 = df3.rename(columns={\"2023.4-2024.3\": \"Time\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate\n",
    "concated_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "concated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns of concated_df\n",
    "renamed_df = concated_df.rename(columns={\"Unnamed: 1\": \"PYQ_Text\", \"Unnamed: 2\": \"Title\", \"Unnamed: 3\": \"Views\", \"Unnamed: 4\": \"reposted\"})\n",
    "renamed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null value with 0 in repetition\n",
    "renamed_df[\"reposted\"] = renamed_df[\"reposted\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to handle instances with null text?\n",
    "renamed_df[renamed_df[\"PYQ_Text\"].isna()]\n",
    "renamed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export and read back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df.to_csv(\"../data/curated/cleaned_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/curated/cleaned_df.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete drop features\n",
    "df.drop(columns='Time', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji from title\n",
    "df['Title_without_emoji'] = df['Title'].apply(remove_emoji)\n",
    "df['PYQ_Text_without_emoji'] = df['PYQ_Text'].apply(remove_emoji)\n",
    "df.drop(columns=['PYQ_Text', 'Title'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop '\\n' and '\\t' in PYQ_Text_without_emoji and Small Title\n",
    "df['PYQ_Text_without_emoji'] = df['PYQ_Text_without_emoji'].str.replace('\\n', ' ')\n",
    "df['PYQ_Text_without_emoji'] = df['PYQ_Text_without_emoji'].str.replace('\\t', ' ')\n",
    "df['Title_without_emoji'] = df['Title_without_emoji'].str.replace('\\n', ' ')\n",
    "df['Title_without_emoji'] = df['Title_without_emoji'].str.replace('\\t', ' ')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label type 2 which is raw rank value from 0 to 1\n",
    "df['QuantileLabel'] = df['Views'].rank(pct=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null PYQ Text with nan\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/curated/df_engineered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"shenzhi-wang/Llama3-8B-Chinese-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = MODEL.replace(\"/\", \"_\")\n",
    "\n",
    "os.makedirs(f\"../data/curated/{model_save_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = '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' #TODO: use cssa account to get new permanant token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_auth_token=token)\n",
    "\n",
    "# Load pre-trained LLaMA model and tokenizer\n",
    "model = LlamaModel.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list: list) -> list:\n",
    "    \"\"\" Converts a list of texts into embeddings using the LLaMA model \"\"\"\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for text in tqdm(text_list):\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items() if key != 'token_type_ids'}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state.cpu().numpy()\n",
    "        all_embeddings.append(embeddings.mean(axis=1))\n",
    "\n",
    "    return np.array(all_embeddings).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_and_save(embeddings: list, filename: str):\n",
    "    np.save(filename, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyq_text = df['PYQ_Text_without_emoji'].values\n",
    "title_text = df['Title_without_emoji'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyq_text_embeddings = get_embeddings(pyq_text)\n",
    "title_embeddings = get_embeddings(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_numpy_and_save(pyq_text_embeddings, f'../data/curated/{model_save_name}/pyq_text_embeddings.npy')\n",
    "to_numpy_and_save(title_embeddings, f'../data/curated/{model_save_name}/title_embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id, val_test_id = train_test_split(range(236), test_size=0.3, random_state=SEED)\n",
    "val_id, test_id = train_test_split(val_test_id, test_size=0.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pyq_text_embeddings = pyq_text_embeddings[train_id]\n",
    "train_title_embeddings = title_embeddings[train_id]\n",
    "train_labels = df['QuantileLabel'].values[train_id]\n",
    "\n",
    "val_pyq_text_embeddings = pyq_text_embeddings[val_id]\n",
    "val_title_embeddings = title_embeddings[val_id]\n",
    "val_labels = df['QuantileLabel'].values[val_id]\n",
    "\n",
    "test_pyq_text_embeddings = pyq_text_embeddings[test_id]\n",
    "test_title_embeddings = title_embeddings[test_id]\n",
    "test_labels = df['QuantileLabel'].values[test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pyq_text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca(embeddings, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(embeddings)\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(embeddings: np.array, feature_type: str) -> pd.DataFrame:\n",
    "    return pd.DataFrame(embeddings, columns=[f'{feature_type}_{i}' for i in range(embeddings.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyq_text_pca = get_pca(train_pyq_text_embeddings, 32)\n",
    "title_pca = get_pca(train_title_embeddings, 32)\n",
    "\n",
    "train_pyq_text_embeddings_pca = pyq_text_pca.transform(train_pyq_text_embeddings)\n",
    "train_title_embeddings_pca = title_pca.transform(train_title_embeddings)\n",
    "\n",
    "val_pyq_text_embeddings_pca = pyq_text_pca.transform(val_pyq_text_embeddings)\n",
    "val_title_embeddings_pca = title_pca.transform(val_title_embeddings)\n",
    "\n",
    "test_pyq_text_embeddings_pca = pyq_text_pca.transform(test_pyq_text_embeddings)\n",
    "test_title_embeddings_pca = title_pca.transform(test_title_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn them into dataframe\n",
    "train_pyq_text_embeddings_pca_df = get_dataframe(train_pyq_text_embeddings_pca, 'pyq_text')\n",
    "train_title_embeddings_pca_df = get_dataframe(train_title_embeddings_pca, 'title')\n",
    "\n",
    "val_pyq_text_embeddings_pca_df = get_dataframe(val_pyq_text_embeddings_pca, 'pyq_text')\n",
    "val_title_embeddings_pca_df = get_dataframe(val_title_embeddings_pca, 'title')\n",
    "\n",
    "test_pyq_text_embeddings_pca_df = get_dataframe(test_pyq_text_embeddings_pca, 'pyq_text')\n",
    "test_title_embeddings_pca_df = get_dataframe(test_title_embeddings_pca, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pyq_text_embeddings_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_both = pd.concat([train_pyq_text_embeddings_pca_df, train_title_embeddings_pca_df], axis=1)\n",
    "train_dataset_both['label'] = train_labels\n",
    "\n",
    "val_dataset_both = pd.concat([val_pyq_text_embeddings_pca_df, val_title_embeddings_pca_df], axis=1)\n",
    "val_dataset_both['label'] = val_labels\n",
    "\n",
    "test_dataset_both = pd.concat([test_pyq_text_embeddings_pca_df, test_title_embeddings_pca_df], axis=1)\n",
    "test_dataset_both['label'] = test_labels\n",
    "\n",
    "train_dataset_both.to_csv(f'../data/curated/{model_save_name}/train_dataset_title_pyq.csv', index=False)\n",
    "val_dataset_both.to_csv(f'../data/curated/{model_save_name}/val_dataset_title_pyq.csv', index=False)\n",
    "test_dataset_both.to_csv(f'../data/curated/{model_save_name}/test_dataset_title_pyq.csv', index=False)\n",
    "\n",
    "with open(f'../data/curated/{model_save_name}/pca_pyq_text.pkl', 'wb') as f:\n",
    "    pickle.dump(pyq_text_pca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_title = pd.concat([train_title_embeddings_pca_df], axis=1)\n",
    "train_dataset_title['label'] = train_labels\n",
    "\n",
    "val_dataset_title = pd.concat([val_title_embeddings_pca_df], axis=1)\n",
    "val_dataset_title['label'] = val_labels\n",
    "\n",
    "test_dataset_title = pd.concat([test_title_embeddings_pca_df], axis=1)\n",
    "test_dataset_title['label'] = test_labels\n",
    "\n",
    "train_dataset_title.to_csv(f'../data/curated/{model_save_name}/train_dataset_title.csv', index=False)\n",
    "val_dataset_title.to_csv(f'../data/curated/{model_save_name}/val_dataset_title.csv', index=False)\n",
    "test_dataset_title.to_csv(f'../data/curated/{model_save_name}/test_dataset_title.csv', index=False)\n",
    "\n",
    "with open(f'../data/curated/{model_save_name}/pca_title.pkl', 'wb') as f:\n",
    "    pickle.dump(title_pca, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
